{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d352852e-d0e0-437a-8102-73becad7459e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL_Examples\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 5000, 2023),\n",
    "    (2, \"Bob\", \"IT\", 6000, 2024),\n",
    "    (3, \"Cathy\", \"IT\", 7500, 2024),\n",
    "    (4, \"David\", \"HR\", 4000, 2023),\n",
    "    (5, \"Eva\", \"Finance\", 8000, 2024)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"dept\", \"salary\", \"year\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "df.where(df.dept == \"IT\").display()\n",
    "spark.sql(\"SELECT * FROM employees WHERE dept = 'IT'\").show()\n",
    "\n",
    "df.orderBy(df.salary.desc()).display()\n",
    "spark.sql(\"SELECT * FROM employees ORDER BY salary DESC\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ff1be5-2288-4647-b60e-0c02688f8dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"name\", \"dept\").show()\n",
    "spark.sql(\"SELECT name, dept FROM employees\").show()\n",
    "df.filter(df.salary>5000).show()\n",
    "spark.sql(\"SELECT * FROM employees WHERE salary > 5000\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96128f92-83ff-47d1-85b1-54c2f330b759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Groupby + aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d66c3fb-98a9-40fc-9a3b-1e4d9f1efc63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"dept\").agg(sum(\"salary\").alias(\"total_salary\")).show()\n",
    "spark.sql(\"SELECT dept, SUM(salary) AS total_salary FROM employees GROUP BY dept\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b855d272-5e45-4976-b16a-a0459169d92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd76428a-f0c0-407a-8f91-fd33c2ad49ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dept_data = [(\"HR\", \"Hyderabad\"), (\"IT\", \"Bangalore\"), (\"Finance\", \"Mumbai\")]\n",
    "dept_df = spark.createDataFrame(dept_data, [\"dept_name\", \"location\"])\n",
    "dept_df.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "df.join(dept_df, df.dept == dept_df.dept_name, \"inner\").select(\"name\", \"dept\", \"location\").show()\n",
    "spark.sql(\"SELECT e.name, e.dept, d.location FROM employees e JOIN departments d ON e.dept = d.dept_name\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a885b0a8-abe1-4cc1-9b2c-2786adbbac4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6250e3c-bf84-491a-9054-6e98a5fa8e43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_2023 = df.filter(df.year == 2023)\n",
    "df_2024 = df.filter(df.year == 2024)\n",
    "# df_2023.union(df_2024).distinct().show()\n",
    "spark.sql(\"SELECT * FROM employees WHERE year=2023 UNION SELECT * FROM employees WHERE year=2024\").display()\n",
    "# df_2023.unionAll(df_2024).show()\n",
    "spark.sql(\"SELECT * FROM employees WHERE year=2023 UNION ALL SELECT * FROM employees WHERE year=2024\").display()\n",
    "# df_2023.intersect(df_2024).show()\n",
    "spark.sql(\"SELECT * FROM employees WHERE year=2023 INTERSECT SELECT * FROM employees WHERE year=2024\").display()\n",
    "# df_2023.exceptAll(df_2024).show()\n",
    "spark.sql(\"SELECT * FROM employees WHERE year=2023 EXCEPT SELECT * FROM employees WHERE year=2024\").display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark sql",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
